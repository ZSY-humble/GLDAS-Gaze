# -*- coding: utf-8 -*-
"""
@Author  : zsy
@Time    : 2025/5/19 ä¸‹åˆ9:01
@File    : metrics.py
@Desc    : compute_info_gain compute_NSS compute_cAUC get_seq_score get_semantic_seq_score
"""
import scipy.ndimage as filters
import numpy as np
import torch
import gzip
from os.path import join
# å¦‚æœ multimatch.py åœ¨ common ç›®å½•ä¸‹
from common.multimatch import docomparison

def multimatch(s1, s2, im_size):
    s1x = s1['X']
    s1y = s1['Y']
    l1 = len(s1x)
    if l1 < 3:
        scanpath1 = np.ones((3, 3), dtype=np.float32)
        scanpath1[:l1, 0] = s1x
        scanpath1[:l1, 1] = s1y
    else:
        scanpath1 = np.ones((l1, 3), dtype=np.float32)
        scanpath1[:, 0] = s1x
        scanpath1[:, 1] = s1y
    s2x = s2['X']
    s2y = s2['Y']
    l2 = len(s2x)
    if l2 < 3:
        scanpath2 = np.ones((3, 3), dtype=np.float32)
        scanpath2[:l2, 0] = s2x
        scanpath2[:l2, 1] = s2y
    else:
        scanpath2 = np.ones((l2, 3), dtype=np.float32)
        scanpath2[:, 0] = s2x
        scanpath2[:, 1] = s2y
    mm = docomparison(scanpath1, scanpath2, sz=im_size)
    return mm[0]


def compute_mm(human_trajs, model_trajs, im_w, im_h, tasks=None):
    """
    compute scanpath similarity using multimatch
    """
    all_mm_scores = []
    for traj in model_trajs:
        img_name = traj['name']
        task = traj['task']
        gt_trajs = list(
            filter(lambda x: x['name'] == img_name and x['task'] == task,
                   human_trajs))
        all_mm_scores.append((task,
                              np.mean([
                                  multimatch(traj, gt_traj, (im_w, im_h))[:4]
                                  for gt_traj in gt_trajs
                              ],
                                      axis=0)))

    if tasks is not None:
        mm_tasks = {}
        for task in tasks:
            mm = np.array([x[1] for x in all_mm_scores if x[0] == task])
            mm_tasks[task] = np.mean(mm, axis=0)
        return mm_tasks
    else:
        return np.mean([x[1] for x in all_mm_scores], axis=0)

        
def compute_info_gain(predicted_probs, gt_fixs, base_probs, eps=2.2204e-16):
    """
    ä¸€ä¸ªè®¡ç®—ã€Œä¿¡æ¯å¢ç›Šï¼ˆInformation Gain, IGï¼‰ã€ çš„å‡½æ•°ï¼Œå®ƒé€šå¸¸ç”¨äºè¯„ä¼°æ³¨è§†ç‚¹é¢„æµ‹æ¨¡å‹åˆ°åº•æ¯”ã€ŒåŸºçº¿æ¨¡å‹ã€å¥½å¤šå°‘ã€‚
    è®¡ç®—ä¿¡æ¯å¢ç›Šï¼ˆInformation Gain, IGï¼‰
    æƒ³è±¡ä½ åœ¨çœ‹ä¸€å¼ å›¾ç‰‡ï¼Œä¸€åªçŒ«ç«™åœ¨ç”»é¢ä¸­é—´ã€‚æˆ‘ä»¬æœ‰ä¸¤ä¸ªæ¨¡å‹ï¼š
    é¢„æµ‹æ¨¡å‹ï¼ˆpredicted_probsï¼‰ï¼šå®ƒæ ¹æ®å›¾åƒå†…å®¹åˆ¤æ–­ä½ æœ€æœ‰å¯èƒ½çœ‹çŒ«ï¼ˆä¸­å¿ƒé™„è¿‘ï¼‰ã€‚
    åŸºçº¿æ¨¡å‹ï¼ˆbase_probsï¼‰ï¼šå®ƒä¸çœ‹å›¾ï¼ŒåªçŸ¥é“äººç±»ä¸€èˆ¬çœ‹å›¾ä¸­å¿ƒï¼ˆcenter biasï¼‰ã€‚
    å‚æ•°ï¼š
    - predicted_probs: Tensorï¼Œå½¢çŠ¶ä¸º (batch_size, H, W)
        æ¨¡å‹é¢„æµ‹çš„æ³¨è§†æ¦‚ç‡åˆ†å¸ƒï¼ŒæŒ‰ç©ºé—´ä½ç½®ç»™å‡ºæ¦‚ç‡
    - gt_fixs: Tensorï¼Œå½¢çŠ¶ä¸º (batch_size, 2)
        çœŸå®æ³¨è§†ç‚¹åæ ‡ (x, y)ï¼Œè¿™é‡Œå‡è®¾ç¬¬0ç»´æ˜¯batchç´¢å¼•ï¼Œç¬¬1ç»´æ˜¯åæ ‡
    - base_probs: Tensorï¼Œå½¢çŠ¶åŒ predicted_probs
        åŸºçº¿æ¦‚ç‡åˆ†å¸ƒï¼Œæ¯”å¦‚å‡åŒ€åˆ†å¸ƒæˆ–è€…ä¸­å¿ƒåç½®æ¨¡å‹çš„æ¦‚ç‡
    - eps: æµ®ç‚¹æ•°ï¼Œå°å¸¸æ•°ï¼Œé¿å…å¯¹æ•°è®¡ç®—æ—¶å‡ºç°log(0)

    è¿”å›ï¼š
    - IG: æ ‡é‡ Tensorï¼Œä¿¡æ¯å¢ç›Šæ€»å’Œ
    """
    # å–çœŸå®æ³¨è§†ç‚¹å¯¹åº”çš„é¢„æµ‹æ¦‚ç‡å€¼ï¼šä» predicted_probs ä¸­æå–æ¯ä¸ªæ ·æœ¬çœŸå®æ³¨è§†ç‚¹å¤„çš„æ¦‚ç‡ ä¸€æ¬¡æ€§ä»ä¸€æ‰¹å›¾åƒçš„é¢„æµ‹çƒ­å›¾ä¸­ï¼Œå–å‡ºæ¯å¼ å›¾ã€Œä½ å®é™…æ³¨è§†ç‚¹ã€å¤„çš„æ¦‚ç‡å€¼ã€‚
    fired_probs = predicted_probs[torch.arange(gt_fixs.size(0)), gt_fixs[:, 1], gt_fixs[:, 0]]

    # å–çœŸå®æ³¨è§†ç‚¹å¯¹åº”çš„åŸºçº¿æ¦‚ç‡å€¼
    fired_base_probs = base_probs[torch.arange(gt_fixs.size(0)), gt_fixs[:, 1], gt_fixs[:, 0]]

    # è®¡ç®—æ¯ä¸ªçœŸå®æ³¨è§†ç‚¹ä¸Šçš„log2æ¦‚ç‡å·®ï¼Œç´¯åŠ å¾—åˆ°æ€»ä¿¡æ¯å¢ç›Š
    IG = torch.sum(torch.log2(fired_probs + eps) - torch.log2(fired_base_probs + eps))

    return IG


def compute_NSS(saliency_map, gt_fixs):

    # NSSè¡¡é‡çš„æ˜¯ï¼šæ¨¡å‹åœ¨çœŸå®æ³¨è§†ç‚¹ä½ç½®ä¸Šçš„é¢„æµ‹æ¦‚ç‡ï¼Œæ¯”æ•´å¼ å›¾çš„å¹³å‡æ°´å¹³é«˜å‡ºå¤šå°‘ä¸ªæ ‡å‡†å·®ã€‚
    # æƒ³è±¡ä½ çœ¼å‰æœ‰ä¸€å¼ çƒ­åŠ›å›¾ï¼ŒæŸäº›åŒºåŸŸå‘äº®ï¼Œè¡¨ç¤ºæ¨¡å‹è§‰å¾—é‚£â€œå¾ˆæ˜¾è‘—â€ã€‚è€Œä½ å®é™…ä¸Šçœ‹å‘äº†æŸä¸€ä¸ªç‚¹ã€‚

    # NSS å°±æ˜¯åœ¨é—®ï¼šâ€œä½ çœ‹çš„é‚£ä¸ªç‚¹ï¼Œåœ¨è¿™å¼ çƒ­å›¾ä¸Šï¼Œæ˜¯äº®çš„ï¼Œè¿˜æ˜¯å¹³æ·¡æ— å¥‡çš„ï¼Ÿâ€
    # çœŸæ­£æ³¨è§†çš„åœ°æ–¹ï¼Œæ˜¯å¦è½åœ¨äº†æ¨¡å‹é¢„æµ‹æ˜¾è‘—åŒºåŸŸï¼ˆçƒ­å›¾çš„â€œäº®ç‚¹â€ï¼‰ä¸Šï¼Œè½å¾—è¶Šå‡†ï¼ŒNSSè¶Šé«˜ã€‚

    # saliency_map: Tensorï¼Œå½¢çŠ¶ä¸º (batch_size, H, W)ï¼Œæ¨¡å‹é¢„æµ‹çš„æ˜¾è‘—å›¾æ¦‚ç‡å€¼
    # gt_fixs: Tensorï¼Œå½¢çŠ¶ä¸º (batch_size, 2)ï¼ŒçœŸå®æ³¨è§†ç‚¹åæ ‡ (x, y)

    # è®¡ç®—æ¯ä¸ªæ ·æœ¬æ˜¾è‘—å›¾çš„å‡å€¼ï¼ˆå±•å¹³åæŒ‰è¡Œè®¡ç®—ï¼‰
    mean = saliency_map.view(gt_fixs.size(0), -1).mean(dim=1)

    # è®¡ç®—æ¯ä¸ªæ ·æœ¬æ˜¾è‘—å›¾çš„æ ‡å‡†å·®ï¼ˆå±•å¹³åæŒ‰è¡Œè®¡ç®—ï¼‰
    std = saliency_map.view(gt_fixs.size(0), -1).std(dim=1)

    # é˜²æ­¢æ ‡å‡†å·®ä¸º0ï¼Œé¿å…åç»­é™¤æ³•å‡ºé”™
    std[std == 0] = 1

    # å–å‡ºæ¯ä¸ªæ ·æœ¬çœŸå®æ³¨è§†ç‚¹å¯¹åº”çš„æ˜¾è‘—å›¾æ•°å€¼ï¼ˆæ¦‚ç‡å€¼ï¼‰
    value = saliency_map[torch.arange(gt_fixs.size(0)), gt_fixs[:, 1], gt_fixs[:, 0]]

    # å¯¹å–å‡ºçš„å€¼åšå½’ä¸€åŒ–å¤„ç†ï¼šå‡å»å‡å€¼ï¼Œé™¤ä»¥æ ‡å‡†å·®
    value -= mean
    value /= std

    # è¿”å›æ‰€æœ‰æ ·æœ¬å½’ä¸€åŒ–å€¼çš„å’Œï¼ˆæ€» NSS å€¼ï¼‰
    return value.sum()

def zero_one_similarity(a, b):
    if a == b:
        return 1.0
    else:
        return 0.0

        
def nw_matching(pred_string, gt_string, gap=0.0):
    """
    nw_matching() æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆæˆ–æ³¨è§†è·¯å¾„ï¼‰ç›¸ä¼¼åº¦æ‰“åˆ†å‡½æ•°ï¼Œè€ƒè™‘äº†åŒ¹é…ã€æ’å…¥ã€åˆ é™¤ä¸‰ç§æ“ä½œï¼Œå¹¶ç”¨åŠ¨æ€è§„åˆ’æ‰¾å‡ºæœ€ä¼˜å¯¹é½è·¯å¾„ï¼Œæœ€ç»ˆè¾“å‡ºå½’ä¸€åŒ–ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆ0~1ï¼‰ã€‚
    """
    # åˆå§‹åŒ–åŠ¨æ€è§„åˆ’çŸ©é˜µFï¼Œå¤§å°ä¸º(len(pred_string)+1, len(gt_string)+1)
    F = np.zeros((len(pred_string) + 1, len(gt_string) + 1), dtype=np.float32)

    # åˆå§‹åŒ–ç¬¬ä¸€åˆ—ï¼Œè¡¨ç¤ºå¯¹pred_stringåºåˆ—çš„iä¸ªå…ƒç´ å…¨åˆ é™¤ï¼ˆæˆ–æ’å…¥gapæƒ©ç½šï¼‰
    for i in range(1 + len(pred_string)):
        F[i, 0] = gap * i

    # åˆå§‹åŒ–ç¬¬ä¸€è¡Œï¼Œè¡¨ç¤ºå¯¹gt_stringåºåˆ—çš„jä¸ªå…ƒç´ å…¨åˆ é™¤ï¼ˆæˆ–æ’å…¥gapæƒ©ç½šï¼‰
    for j in range(1 + len(gt_string)):
        F[0, j] = gap * j

    # åŠ¨æ€è§„åˆ’å¡«è¡¨
    for i in range(1, 1 + len(pred_string)):
        for j in range(1, 1 + len(gt_string)):
            a = pred_string[i - 1]  # pred_stringå½“å‰å…ƒç´ 
            b = gt_string[j - 1]    # gt_stringå½“å‰å…ƒç´ 

            # è®¡ç®—åŒ¹é…å¾—åˆ†ï¼šå¯¹è§’çº¿å…ƒç´  + å½“å‰ä¸¤ä¸ªå…ƒç´ çš„ç›¸ä¼¼åº¦
            match = F[i - 1, j - 1] + zero_one_similarity(a, b)

            # åˆ é™¤æ“ä½œï¼ˆpred_stringçš„å…ƒç´ è¢«åˆ é™¤ï¼‰
            delete = F[i - 1, j] + gap

            # æ’å…¥æ“ä½œï¼ˆgt_stringçš„å…ƒç´ è¢«æ’å…¥ï¼‰
            insert = F[i, j - 1] + gap

            # å–ä¸‰è€…ä¸­çš„æœ€å¤§å€¼ï¼Œå¡«å…¥F[i,j]
            F[i, j] = np.max([match, delete, insert])

    # å½’ä¸€åŒ–å¾—åˆ†ï¼šç”¨æœ€åä¸€ä¸ªæ ¼å­çš„å€¼é™¤ä»¥è¾ƒé•¿åºåˆ—é•¿åº¦
    score = F[len(pred_string), len(gt_string)]
    return score / max(len(pred_string), len(gt_string))

"""
scanpath2clusters å°†ä¸€ä¸ªæ³¨è§†è·¯å¾„ï¼ˆscanpathï¼‰ä¸­çš„æ‰€æœ‰æ³¨è§†ç‚¹ï¼ˆx, y åæ ‡ï¼‰ä¼ å…¥ä¸€ä¸ª MeanShift èšç±»æ¨¡å‹ ä¸­ï¼Œ
å¾—åˆ°æ¯ä¸ªæ³¨è§†ç‚¹æ‰€å±çš„èšç±»æ ‡ç­¾ï¼Œç„¶åå°†è¿™äº›æ ‡ç­¾ç»„æˆä¸€ä¸ªåºåˆ—ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰ï¼Œç”¨äºåç»­çš„è¡Œä¸ºåˆ†ææˆ–åŒ¹é…æ¯”å¯¹ï¼ˆå¦‚NWåŒ¹é…ï¼‰ã€‚
"""
def scanpath2clusters(meanshift, scanpath):
    """

    :param meanshift: ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„ MeanShift èšç±»æ¨¡å‹
    :param scanpath:ä¸€ä¸ªå­—å…¸ç±»å‹ï¼ŒåŒ…å« Xã€Y åæ ‡åºåˆ—ï¼ˆçœ¼åŠ¨æ³¨è§†ç‚¹è½¨è¿¹ï¼‰
    :return: scanpathçš„èšç±»æ ‡ç­¾åºåˆ—
    """
    string = []  # ç”¨äºå­˜æ”¾æ¯ä¸ªæ³¨è§†ç‚¹å¯¹åº”çš„èšç±»æ ‡ç­¾ï¼ˆç±»åˆ«ç¬¦å·ï¼‰
    xs = scanpath['X']  # çœ¼åŠ¨è½¨è¿¹çš„Xåæ ‡åºåˆ—
    ys = scanpath['Y']  # çœ¼åŠ¨è½¨è¿¹çš„Yåæ ‡åºåˆ—

    # éå†æ¯ä¸ªæ³¨è§†ç‚¹çš„åæ ‡
    for i in range(len(xs)):
        # ç”¨meanshiftèšç±»æ¨¡å‹é¢„æµ‹å½“å‰æ³¨è§†ç‚¹æ‰€å±çš„èšç±»æ ‡ç­¾
        symbol = meanshift.predict([[xs[i], ys[i]]])[0]
        string.append(symbol)  # æŠŠæ ‡ç­¾æ·»åŠ è¿›ç»“æœåˆ—è¡¨

    return string  # è¿”å›è¯¥scanpathçš„èšç±»æ ‡ç­¾åºåˆ—

def compute_SS(preds, clusters, truncate, truncate_gt, reduce='mean'):
    """
    ç”¨æ¥è®¡ç®—é¢„æµ‹çš„çœ¼åŠ¨æ‰«æè·¯å¾„ï¼ˆscanpathï¼‰å’ŒçœŸå®ç±»åˆ«åºåˆ—ï¼ˆground truth clustersï¼‰ä¹‹é—´çš„ç›¸ä¼¼åº¦è¯„åˆ†ï¼Œ
    æ ¸å¿ƒæ€æƒ³æ˜¯æŠŠçœ¼åŠ¨è½¨è¿¹è½¬æˆç±»åˆ«åºåˆ—ï¼Œå†ç”¨åºåˆ—æ¯”å¯¹ç®—æ³•è®¡ç®—åŒ¹é…åº¦ï¼Œæœ€åè¿”å›æ¯æ¡æ‰«æè·¯å¾„çš„ç›¸ä¼¼åº¦ç»“æœã€‚
    æƒ³è±¡ä½ æœ‰ä¸€å †çœ¼åŠ¨è·¯å¾„æ•°æ®ï¼ˆæ¯ä¸ªäººçœ‹ä¸œè¥¿æ—¶çœ¼ç›çš„è·³åŠ¨è½¨è¿¹ï¼‰ï¼Œä½ å…ˆæŠŠæ¯æ¡è·¯å¾„ä¸Šçš„ç‚¹åˆ†åˆ°ä¸åŒçš„åŒºåŸŸç±»åˆ«ï¼ˆç”¨èšç±»ç®—æ³•åˆ†ç±»ï¼‰ã€‚ç„¶åä½ æœ‰å¯¹åº”çš„çœŸå®â€œå‚è€ƒç­”æ¡ˆâ€â€”â€”æ­£ç¡®çš„ç±»åˆ«åºåˆ—ã€‚
    ä½ æƒ³çŸ¥é“ä½ çš„é¢„æµ‹è½¨è¿¹å’ŒçœŸå®å‚è€ƒæœ‰å¤šåƒï¼Œå°±ç”¨åºåˆ—æ¯”å¯¹ç®—æ³•ï¼ˆnw_matchingï¼‰æ¯”è¾ƒå®ƒä»¬çš„ç›¸ä¼¼åº¦ã€‚æœ€åä½ æŠŠè¿™äº›ç›¸ä¼¼åº¦ç»Ÿè®¡èµ·æ¥ï¼Œæ¯”å¦‚æ±‚å¹³å‡ï¼Œæ¥è¯„ä»·æ•´ä½“é¢„æµ‹æ•ˆæœã€‚
    è§£å†³ä¸€ä¸ªé¢„æµ‹è·¯å¾„å’Œå¤šä¸ªå‚è€ƒç­”æ¡ˆä¹‹é—´çš„æ¯”è¾ƒé—®é¢˜ã€‚
    ä¸‹é¢ç»™ä½ å½¢è±¡è¯¦ç»†è®²è§£æ¯æ­¥çš„ä½œç”¨ï¼š
    è¾“å…¥ï¼š
    predsï¼šé¢„æµ‹çš„æ‰«æè·¯å¾„åˆ—è¡¨ï¼Œæ¯æ¡æ‰«æè·¯å¾„æ˜¯ä¸ªå­—å…¸ï¼Œé‡Œé¢æœ‰æ¡ä»¶ã€ä»»åŠ¡åã€è·¯å¾„åå­—ç­‰ä¿¡æ¯ã€‚
    clustersï¼šèšç±»ç»“æœï¼Œä¿å­˜äº†æ¯æ¡è·¯å¾„å¯¹åº”çš„çœŸå®ç±»åˆ«å­—ç¬¦ä¸²å’Œé¢„æµ‹çš„ç±»åˆ«åºåˆ—ã€‚
    truncateï¼šæˆªæ–­é•¿åº¦ï¼Œæœ€é•¿å¯¹æ¯”å¤šå°‘æ­¥ã€‚
    truncate_gtï¼šæ˜¯å¦ä¹Ÿå¯¹çœŸå®ç±»åˆ«åºåˆ—æˆªæ–­ã€‚
    reduceï¼šå¯¹å¤šä¸ªç›¸ä¼¼åº¦å¾—åˆ†å¦‚ä½•æ±‡æ€»ï¼ˆå¹³å‡æˆ–æœ€å¤§ï¼‰ã€‚
    è¾“å‡ºï¼š
    {
    'condition': 'freeview' æˆ– 'TP',
    'task': ä»»åŠ¡åï¼ˆå¦‚æœæœ‰ä»»åŠ¡ï¼‰,
    'name': 'å›¾åƒæ–‡ä»¶å',
    'score': ä¸å¤šä¸ª ground truth è·¯å¾„åŒ¹é…çš„å¹³å‡ç›¸ä¼¼åº¦å¾—åˆ†
    }
    """
    results = []
    # éå†æ¯æ¡é¢„æµ‹æ‰«æè·¯å¾„
    for scanpath in preds:
        # åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªç”±æµè§ˆï¼ˆfreeviewï¼‰æ¡ä»¶
        is_fv = scanpath['condition'] == 'freeview'

        # æ„é€ clustersçš„keyï¼ŒåŒºåˆ†freeviewå’Œtaskæ¡ä»¶
        if is_fv:
            key = 'test-{}-{}'.format(scanpath['condition'], scanpath['name'].split('.')[0])
        else:
            key = 'test-{}-{}-{}'.format(scanpath['condition'], scanpath['task'],
                                         scanpath['name'].split('.')[0])

        # è·å–å¯¹åº”keyçš„clustersä¿¡æ¯
        ms = clusters[key]
        strings = ms['strings']  # å¤šä¸ª ground truth ç±»åˆ«åºåˆ— â€”â€” å³å¤šä¸ªâ€œæ­£ç¡®ç­”æ¡ˆâ€ã€‚
        cluster = ms['cluster']  # é¢„æµ‹è·¯å¾„å¯¹åº”çš„ç±»åˆ«åºåˆ—

        # å°†é¢„æµ‹è·¯å¾„æ˜ å°„æˆç±»åˆ«åºåˆ—
        pred = scanpath2clusters(cluster, scanpath)

        scores = []
        # è‹¥æ— gtå­—ç¬¦ä¸²ï¼Œè·³è¿‡
        if len(strings) == 0:
            continue

        # éå†æ‰€æœ‰gtç±»åˆ«å­—ç¬¦ä¸²ï¼Œè®¡ç®—ç›¸ä¼¼åº¦ è®©è¿™ä¸€æ¡é¢„æµ‹è·¯å¾„ï¼Œåˆ†åˆ«ä¸å¤šä¸ªæ­£ç¡®ç­”æ¡ˆå¯¹æ¯”ï¼Œå¾—å‡ºå¤šä¸ªåˆ†æ•°ï¼Œç„¶åå–å¹³å‡ä½œä¸ºæœ€ç»ˆå¾—åˆ†ã€‚
        for gt in strings:
            if len(gt) > 0:
                # æ ¹æ®truncateå‚æ•°æˆªæ–­é¢„æµ‹åºåˆ—
                pred = pred[:truncate] if len(pred) > truncate else pred
                # æ ¹æ®truncate_gtå‚æ•°æˆªæ–­gtåºåˆ—
                if truncate_gt:
                    gt = gt[:truncate] if len(gt) > truncate else gt

                # è®¡ç®—nw_matchingï¼ˆNeedleman-Wunschåºåˆ—åŒ¹é…ï¼‰å¾—åˆ†
                score = nw_matching(pred, gt)
                scores.append(score)

        # æ„å»ºå•æ¡scanpathçš„ç»“æœå­—å…¸
        result = {}
        result['condition'] = scanpath['condition']
        if not is_fv:
            result['task'] = scanpath['task']
        result['name'] = scanpath['name']

        # å¯¹scoresåˆ—è¡¨åšé™ç»´å¤„ç†ï¼Œé»˜è®¤å–å¹³å‡ï¼Œä¹Ÿå¯å–æœ€å¤§
        if reduce == 'mean':
            result['score'] = np.array(scores).mean()
        elif reduce == 'max':
            result['score'] = max(scores)
        else:
            raise NotImplementedError

        results.append(result)

    return results


def compute_SSS(preds,
                fixations,
                truncate,
                segmentation_map_dir,
                truncate_gt,
                reduce='mean'):
    """
    è®¡ç®—é¢„æµ‹çš„çœ¼åŠ¨æ‰«æè·¯å¾„ï¼ˆscanpathï¼‰ä¸çœŸå®è¯­ä¹‰ç±»åˆ«åºåˆ—ä¹‹é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚

    âœ… ä¸ compute_SS çš„æ ¸å¿ƒåŒºåˆ«ï¼š
        - compute_SS ä½¿ç”¨çš„æ˜¯èšç±»åçš„â€œç±»åˆ«IDåºåˆ—â€ï¼›
        - compute_SSS ä½¿ç”¨çš„æ˜¯â€œåˆ†å‰²å›¾è¯­ä¹‰æ ‡ç­¾â€ï¼Œå³ç›´æ¥ä»çœŸå®å›¾ç‰‡çš„ segmentation map ä¸­æå–çš„ç±»åˆ«æ ‡ç­¾åºåˆ—ã€‚
    ä¸¤è€…éƒ½æ˜¯åœ¨åšä¸€ä»¶äº‹ï¼š
    æ¯”è¾ƒé¢„æµ‹çš„çœ¼åŠ¨è·¯å¾„ï¼ˆscanpathï¼‰å’Œâ€œæŸç§å½¢å¼â€çš„çœŸå®å‚è€ƒè·¯å¾„ï¼ˆground truthï¼‰ï¼Œçœ‹å®ƒä»¬æœ‰å¤šåƒã€‚
    ä½†å…³é”®åœ¨äºï¼š
    å‚è€ƒè·¯å¾„ï¼ˆground truthï¼‰åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿæ€ä¹ˆå¾—æ¥çš„ï¼Ÿ
    âœ… compute_SSï¼šèšç±»åŒºåŸŸæ¯”å¯¹
ğŸ§  åƒæ˜¯åœ¨è€ƒä½ â€œçœ¼ç›çœ‹è¿‡å“ªäº›åŒºåŸŸâ€
    ground truth æ˜¯èšç±»åçš„â€œç±»åˆ« ID åºåˆ—â€ï¼š
    æ¯”å¦‚æŠŠå›¾åƒåˆ†æˆ 10 ä¸ªåŒºåŸŸï¼ˆèšç±»ï¼‰ï¼ŒçœŸå®çš„æ³¨è§†è·¯å¾„æ˜¯ [3, 7, 7, 2]ï¼Œè¡¨ç¤ºæ³¨è§†ä¾æ¬¡è½åœ¨è¿™äº›åŒºåŸŸä¸Šã€‚
    é¢„æµ‹è·¯å¾„ä¹Ÿè¢«æ˜ å°„æˆè¿™äº›åŒºåŸŸç¼–å·åºåˆ—ï¼Œæ¯”å¦‚ [3, 7, 2, 2]ã€‚
    ç„¶åæ¯”è¾ƒä¸¤ä¸ªâ€œåŒºåŸŸç¼–å·åºåˆ—â€æœ‰å¤šåƒï¼ˆç”¨åºåˆ—åŒ¹é…ç®—æ³•ï¼‰ã€‚
ğŸ§­ ä¸¾ä¾‹ç±»æ¯”ï¼š
å°±åƒä½ è®©å­¦ç”Ÿåœ¨ä¸€å¼ å›¾ä¸Šè‡ªç”±æµè§ˆï¼Œä½ åªå…³å¿ƒä»–ä»¬çœ‹äº†å“ªäº›â€œåŒºåŸŸâ€ï¼ˆä¸åœ¨ä¹è¿™äº›åŒºåŸŸæ˜¯äººè„¸è¿˜æ˜¯æ¯å­ï¼Œåªçœ‹ç¼–å·ï¼‰ã€‚

âœ… compute_SSSï¼šè¯­ä¹‰åˆ†å‰²æ¯”å¯¹
ğŸ§  åƒæ˜¯åœ¨è€ƒä½ â€œçœ¼ç›æ³¨è§†çš„æ˜¯å“ªäº›è¯­ä¹‰å¯¹è±¡â€
    ground truth æ˜¯å›¾åƒåˆ†å‰²è¯­ä¹‰æ ‡ç­¾ï¼Œæ¯”å¦‚ï¼š
        å›¾åƒä¸Šæ¯ä¸ªåƒç´ æœ‰è¯­ä¹‰ç±»åˆ«æ ‡ç­¾ï¼ˆäººè„¸ = 1ï¼Œæ¯å­ = 2ï¼ŒèƒŒæ™¯ = 0...ï¼‰
        çœŸå®æ³¨è§†ç‚¹æ˜¯ [1, 1, 2, 3]ï¼Œè¡¨ç¤ºä¾æ¬¡æ³¨è§†â€œäººè„¸ã€äººè„¸ã€æ¯å­ã€æ¡Œå­â€ã€‚
    é¢„æµ‹è·¯å¾„ä¹Ÿé€šè¿‡åˆ†å‰²å›¾å¾—åˆ°è¯­ä¹‰ç±»åˆ«åºåˆ—ï¼Œæ¯”å¦‚ [1, 2, 2, 0]ã€‚
    ç„¶ååŒæ ·æ¯”è¾ƒé¢„æµ‹è¯­ä¹‰åºåˆ—å’ŒçœŸå®è¯­ä¹‰åºåˆ—çš„åŒ¹é…åº¦ã€‚
ğŸ§­ ä¸¾ä¾‹ç±»æ¯”ï¼š
å°±åƒä½ è®©å­¦ç”Ÿçœ‹å›¾ï¼Œä½ ä¸ä»…å…³å¿ƒä»–ä»¬çœ‹äº†å“ªé‡Œï¼Œè¿˜å…³å¿ƒä»–ä»¬çœ‹çš„æ˜¯ä¸æ˜¯â€œå…³é”®è¯­ä¹‰ç‰©ä½“â€ï¼ˆå¦‚ç›®æ ‡ã€äººè„¸ç­‰ï¼‰â€”â€”ä¸æ˜¯åªçœ‹åŒºåŸŸç¼–å·ï¼Œè€Œæ˜¯çœ‹å®é™…ç‰©ä½“/æ„ä¹‰ã€‚

    å‚æ•°ï¼š
    - predsï¼šé¢„æµ‹çš„æ‰«æè·¯å¾„åˆ—è¡¨ï¼ˆåŒ…å«åæ ‡ä¿¡æ¯ï¼‰ã€‚
    - fixationsï¼šçœŸå®æ³¨è§†ç‚¹å¯¹åº”çš„è¯­ä¹‰æ ‡ç­¾åºåˆ—ï¼ˆæ¯ä¸ªè·¯å¾„å¯¹åº”å¤šä¸ª ground truth åºåˆ—ï¼‰ã€‚
    - truncateï¼šæˆªæ–­é•¿åº¦ï¼Œé™åˆ¶æ¯”è¾ƒçš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚
    - segmentation_map_dirï¼šåˆ†å‰²å›¾çš„è·¯å¾„ï¼Œæ¯å¼ å›¾éƒ½æ˜¯ `.npy.gz` æ ¼å¼ã€‚
    - truncate_gtï¼šæ˜¯å¦æˆªæ–­ ground truth åºåˆ—ã€‚
    - reduceï¼šå¯¹å¤šä¸ªå¾—åˆ†çš„æ±‡æ€»æ–¹å¼ï¼ˆ'mean' æˆ– 'max'ï¼‰ã€‚
    """
    results = []

    # â¬‡ï¸ å†…éƒ¨å‡½æ•°ï¼šå°†é¢„æµ‹çš„æ³¨è§†è·¯å¾„æ˜ å°„ä¸ºè¯­ä¹‰ç±»åˆ«æ ‡ç­¾åºåˆ—ï¼ˆå¦‚ ['3', '7', '7', '10']ï¼‰
    def scanpath2categories(seg_map, scanpath):
        string = []  # ç”¨æ¥å­˜å‚¨æ¯ä¸ªæ³¨è§†ç‚¹å¯¹åº”çš„ç±»åˆ«æ ‡ç­¾ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰
        xs = scanpath['X']  # X åæ ‡åºåˆ—
        ys = scanpath['Y']  # Y åæ ‡åºåˆ—

        # éå†æ‰€æœ‰æ³¨è§†ç‚¹ æŠŠé¢„æµ‹çš„çœ¼åŠ¨è½¨è¿¹ä¸­çš„æ¯ä¸ªæ³¨è§†ç‚¹ï¼Œæ˜ å°„ä¸ºå®ƒè½åœ¨å›¾åƒä¸Šå¯¹åº”çš„è¯­ä¹‰ç±»åˆ«æ ‡ç­¾ï¼ˆé€šè¿‡åˆ†å‰²å›¾ï¼‰ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªâ€œè¯­ä¹‰ç±»åˆ«åºåˆ—â€ã€‚
        # zipæŠŠä¸¤ä¸ªåºåˆ—â€œæ‰“åŒ…â€æˆä¸€å¯¹å¯¹åæ ‡ (x, y)ï¼Œç”¨äºä¸€èµ·éå†ï¼Œæ¯”å¦‚çœ¼åŠ¨è½¨è¿¹ä¸­çš„æ¯ä¸€ä¸ªæ³¨è§†ç‚¹çš„ä½ç½®ã€‚
        for x, y in zip(xs, ys):
            # è·å–å½“å‰ä½ç½®åœ¨åˆ†å‰²å›¾ä¸Šçš„è¯­ä¹‰ç±»åˆ«ï¼ˆè½¬ä¸ºæ•´æ•°å†è½¬ä¸ºå­—ç¬¦ä¸²ï¼‰
            symbol = str(int(seg_map[int(y), int(x)]))
            string.append(symbol)  # åŠ å…¥å½“å‰ç‚¹çš„è¯­ä¹‰ç±»åˆ«æ ‡ç­¾

        return string  # è¿”å›æ•´ä¸ªscanpathçš„ç±»åˆ«æ ‡ç­¾åºåˆ—

    # â¬‡ï¸ éå†æ¯ä¸ªé¢„æµ‹scanpath
    for scanpath in preds:
        is_fv = scanpath['condition'] == 'freeview'

        # æ„é€ å”¯ä¸€ key æ¥ä» fixations ä¸­æ‰¾åˆ°è¯¥å›¾çš„ ground truth æ³¨è§†æ ‡ç­¾åºåˆ—
        if is_fv:
            key = 'test-{}-{}'.format(scanpath['condition'], scanpath['name'].split('.')[0])
        else:
            key = 'test-{}-{}-{}'.format(scanpath['condition'], scanpath['task'],
                                         scanpath['name'].split('.')[0])

        # ğŸ”¸ è·å–å¯¹åº” key çš„ ground truth ç±»åˆ«å­—ç¬¦ä¸²åˆ—è¡¨
        strings = fixations[key]  # æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå¤šä¸ªçœŸå®æ³¨è§†è·¯å¾„çš„è¯­ä¹‰æ ‡ç­¾åºåˆ—ï¼‰

        # ğŸ”¸ ä»å‹ç¼©æ–‡ä»¶ä¸­è½½å…¥å½“å‰å›¾åƒçš„è¯­ä¹‰åˆ†å‰²å›¾ï¼ˆnpy.gz æ ¼å¼ï¼‰
        with gzip.GzipFile(
                join(segmentation_map_dir, scanpath['name'][:-3] + 'npy.gz'),
                "r") as r:
            segmentation_map = np.load(r, allow_pickle=True)
            r.close()

        # ğŸ”¸ å°†é¢„æµ‹è·¯å¾„åæ ‡è½¬æ¢ä¸ºè¯­ä¹‰ç±»åˆ«æ ‡ç­¾åºåˆ—
        pred = scanpath2categories(segmentation_map, scanpath)

        scores = []  # ä¿å­˜è¯¥é¢„æµ‹è·¯å¾„ä¸æ¯ä¸ª ground truth åŒ¹é…çš„åˆ†æ•°

        # â¬‡ï¸ éå†æ‰€æœ‰ ground truth åºåˆ—ï¼Œä¸é¢„æµ‹è¿›è¡ŒåŒ¹é…æ¯”å¯¹
        for gt in strings:
            if len(gt) > 0:
                # æˆªæ–­é¢„æµ‹åºåˆ—ï¼ˆå¦‚æœå¤ªé•¿ï¼‰
                pred = pred[:truncate] if len(pred) > truncate else pred
                # æˆªæ–­ ground truth åºåˆ—ï¼ˆå¦‚æœå¯ç”¨ truncate_gtï¼‰
                if truncate_gt:
                    gt = gt[:truncate] if len(gt) > truncate else gt

                # ğŸ§® ç”¨ Needleman-Wunsch ç®—æ³•è®¡ç®—ä¸¤ä¸ªåºåˆ—çš„åŒ¹é…å¾—åˆ†
                score = nw_matching(pred, gt)
                scores.append(score)

        # â¬‡ï¸ æ„å»ºå½“å‰è·¯å¾„çš„ç»“æœå­—å…¸
        result = {}
        result['condition'] = scanpath['condition']
        if not is_fv:
            result['task'] = scanpath['task']
        result['name'] = scanpath['name']

        # â¬‡ï¸ æ±‡æ€»å¤šä¸ª ground truth å¾—åˆ†ï¼ˆå¹³å‡æˆ–æœ€å¤§ï¼‰
        if reduce == 'mean':
            result['score'] = np.array(scores).mean()
        elif reduce == 'max':
            result['score'] = max(scores)
        else:
            raise NotImplementedError

        results.append(result)

    return results  # è¿”å›æ‰€æœ‰é¢„æµ‹è·¯å¾„çš„åŒ¹é…å¾—åˆ†ç»“æœ

# 1. ç¼–è¾‘è·ç¦»åŸºç¡€å‡½æ•°
def _Levenshtein_Dmatrix_initializer(len1, len2):
    Dmatrix = []
    for i in range(len1):
        Dmatrix.append([0] * len2)
    for i in range(len1):
        Dmatrix[i][0] = i
    for j in range(len2):
        Dmatrix[0][j] = j
    return Dmatrix

def _Levenshtein_cost_step(Dmatrix, string_1, string_2, i, j, substitution_cost=1):
    char_1 = string_1[i - 1]
    char_2 = string_2[j - 1]
    insertion = Dmatrix[i - 1][j] + 1
    deletion = Dmatrix[i][j - 1] + 1
    substitution = Dmatrix[i - 1][j - 1] + substitution_cost * (char_1 != char_2)
    Dmatrix[i][j] = min(insertion, deletion, substitution)

def _Levenshtein(string_1, string_2, substitution_cost=1):
    len1 = len(string_1)
    len2 = len(string_2)
    Dmatrix = _Levenshtein_Dmatrix_initializer(len1 + 1, len2 + 1)
    
    for i in range(len1):
        for j in range(len2):
            _Levenshtein_cost_step(Dmatrix, string_1, string_2, 
                                   i + 1, j + 1, substitution_cost=substitution_cost)
    
    if substitution_cost == 1:
        max_dist = max(len1, len2)
    elif substitution_cost == 2:
        max_dist = len1 + len2
    
    return Dmatrix[len1][len2]

# 2. EDï¼ˆç¼–è¾‘è·ç¦»ï¼‰è®¡ç®—å‡½æ•°
def compute_ED(preds, clusters, truncate, truncate_gt=False, reduce='mean'):
    results = []
    for scanpath in preds:
        is_fv = scanpath['condition'] == 'freeview'
        if is_fv:
            key = 'test-{}-{}'.format(scanpath['condition'], scanpath['name'].split('.')[0])
        else:
            key = 'test-{}-{}-{}'.format(scanpath['condition'], scanpath['task'],
                                         scanpath['name'].split('.')[0])
        ms = clusters[key]
        strings = ms['strings']
        cluster = ms['cluster']
        
        pred = scanpath2clusters(cluster, scanpath)
        scores = []
        if len(strings) == 0:
            continue
        for gt in strings:
            if len(gt) > 0:
                pred = pred[:truncate] if len(pred) > truncate else pred
                if truncate_gt:
                    gt = gt[:truncate] if len(gt) > truncate else gt
                score = _Levenshtein(pred, gt)
                scores.append(score)
        
        result = {}
        result['condition'] = scanpath['condition']
        if not is_fv:
            result['task'] = scanpath['task']
        result['name'] = scanpath['name']
        if reduce == 'mean':
            result['score'] = np.array(scores).mean()
        elif reduce == 'max':
            result['score'] = max(scores)
        else:
            raise NotImplementedError
        results.append(result)
    return results

def get_ed(preds, clusters, max_step, truncate_gt=False, tasks=None):
    results = compute_ED(preds, clusters, max_step, truncate_gt)
    if tasks is None:
        return np.mean([r['score'] for r in results])
    else:
        scores = []
        for task in tasks:
            scores.append(
                np.mean([r['score'] for r in results if r['task'] == task]))
        return dict(zip(tasks, scores))

# 3. SEDï¼ˆè¯­ä¹‰ç¼–è¾‘è·ç¦»ï¼‰è®¡ç®—å‡½æ•°
def compute_SED(preds, fixations, truncate, segmentation_map_dir, truncate_gt=False, reduce='mean'):
    results = []
    # â¬‡ï¸ å†…éƒ¨å‡½æ•°ï¼šå°†é¢„æµ‹çš„æ³¨è§†è·¯å¾„æ˜ å°„ä¸ºè¯­ä¹‰ç±»åˆ«æ ‡ç­¾åºåˆ—ï¼ˆå¦‚ ['3', '7', '7', '10']ï¼‰
    def scanpath2categories(seg_map, scanpath):
        string = []  # ç”¨æ¥å­˜å‚¨æ¯ä¸ªæ³¨è§†ç‚¹å¯¹åº”çš„ç±»åˆ«æ ‡ç­¾ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰
        xs = scanpath['X']  # X åæ ‡åºåˆ—
        ys = scanpath['Y']  # Y åæ ‡åºåˆ—

        # éå†æ‰€æœ‰æ³¨è§†ç‚¹ æŠŠé¢„æµ‹çš„çœ¼åŠ¨è½¨è¿¹ä¸­çš„æ¯ä¸ªæ³¨è§†ç‚¹ï¼Œæ˜ å°„ä¸ºå®ƒè½åœ¨å›¾åƒä¸Šå¯¹åº”çš„è¯­ä¹‰ç±»åˆ«æ ‡ç­¾ï¼ˆé€šè¿‡åˆ†å‰²å›¾ï¼‰ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªâ€œè¯­ä¹‰ç±»åˆ«åºåˆ—â€ã€‚
        # zipæŠŠä¸¤ä¸ªåºåˆ—â€œæ‰“åŒ…â€æˆä¸€å¯¹å¯¹åæ ‡ (x, y)ï¼Œç”¨äºä¸€èµ·éå†ï¼Œæ¯”å¦‚çœ¼åŠ¨è½¨è¿¹ä¸­çš„æ¯ä¸€ä¸ªæ³¨è§†ç‚¹çš„ä½ç½®ã€‚
        for x, y in zip(xs, ys):
            # è·å–å½“å‰ä½ç½®åœ¨åˆ†å‰²å›¾ä¸Šçš„è¯­ä¹‰ç±»åˆ«ï¼ˆè½¬ä¸ºæ•´æ•°å†è½¬ä¸ºå­—ç¬¦ä¸²ï¼‰
            symbol = str(int(seg_map[int(y), int(x)]))
            string.append(symbol)  # åŠ å…¥å½“å‰ç‚¹çš„è¯­ä¹‰ç±»åˆ«æ ‡ç­¾

        return string  # è¿”å›æ•´ä¸ªscanpathçš„ç±»åˆ«æ ‡ç­¾åºåˆ—
    for scanpath in preds:
        is_fv = scanpath['condition'] == 'freeview'
        if is_fv:
            key = 'test-{}-{}'.format(scanpath['condition'], scanpath['name'].split('.')[0])
        else:
            key = 'test-{}-{}-{}'.format(scanpath['condition'], scanpath['task'],
                                         scanpath['name'].split('.')[0])
        strings = fixations[key]
        
        with gzip.GzipFile(
                join(segmentation_map_dir, scanpath['name'][:-3] + 'npy.gz'), "r") as r:
            segmentation_map = np.load(r, allow_pickle=True)
            r.close()
        
        pred = scanpath2categories(segmentation_map, scanpath)
        scores = []
        for gt in strings:
            if len(gt) > 0:
                pred = pred[:truncate] if len(pred) > truncate else pred
                if truncate_gt:
                    gt = gt[:truncate] if len(gt) > truncate else gt
                score = _Levenshtein(pred, gt)
                scores.append(score)
        
        result = {}
        result['condition'] = scanpath['condition']
        if not is_fv:
            result['task'] = scanpath['task']
        result['name'] = scanpath['name']
        if reduce == 'mean':
            result['score'] = np.array(scores).mean()
        elif reduce == 'max':
            result['score'] = max(scores)
        else:
            raise NotImplementedError
        results.append(result)
    return results

def get_semantic_ed(preds, fixations, max_step, segmentation_map_dir, truncate_gt=False, tasks=None):
    results = compute_SED(preds, fixations, max_step, segmentation_map_dir, truncate_gt)
    if tasks is None:
        return np.mean([r['score'] for r in results])
    else:
        scores = []
        for task in tasks:
            scores.append(
                np.mean([r['score'] for r in results if r['task'] == task]))
        return dict(zip(tasks, scores))


def compute_cAUC(s_map, gt_next_fixs):
    """
    è®¡ç®—åŸºäºæ³¨è§†ç‚¹çš„AUC-JuddæŒ‡æ ‡ï¼Œè¡¡é‡æ˜¾è‘—å›¾ä¸­çœŸå®æ³¨è§†ç‚¹çš„æ˜¾è‘—æ€§å€¼åœ¨æ•´ä½“æ˜¾è‘—å€¼ä¸­çš„ç™¾åˆ†ä½ã€‚
    æ¨¡å‹é¢„æµ‹çš„æ˜¾è‘—åŒºåŸŸæ˜¯å¦è¦†ç›–äº†çœŸæ­£è¢«äººæ³¨è§†çš„ç‚¹ã€‚å®ƒé€šè¿‡ ROC æ›²çº¿ä¸Šçš„ç§¯åˆ†ï¼ˆAUCï¼‰æ¥è¡¨ç¤ºæ˜¾è‘—å›¾çš„â€œå¯ä¿¡åº¦â€ã€‚
    Args:
       s_map: Tensorï¼Œå½¢çŠ¶ä¸º [B, H, W]ï¼Œæ¨¡å‹é¢„æµ‹çš„æ˜¾è‘—å›¾
       gt_next_fixs: Tensorï¼Œå½¢çŠ¶ä¸º [B, 2]ï¼ŒçœŸå®æ³¨è§†ç‚¹çš„(x, y)åæ ‡  2 è¡¨ç¤º æ¯ä¸ªæ³¨è§†ç‚¹çš„ä¸¤ä¸ªåæ ‡å€¼ï¼Œå³ (x, y)ã€‚
    """
    # å¯¹æ¯å¼ å›¾ï¼Œå–å‡ºçœŸå®æ³¨è§†ç‚¹å¯¹åº”çš„æ˜¾è‘—å€¼ä½œä¸ºé˜ˆå€¼
    thresholds = s_map[torch.arange(len(gt_next_fixs)),
    gt_next_fixs[:, 1],
    gt_next_fixs[:, 0]]

    bs = len(gt_next_fixs)  # batch size

    area = []
    area.append(torch.zeros(bs, 2))  # AUCæ›²çº¿èµ·ç‚¹åæ ‡ (0,0)

    # åªä¿ç•™æ˜¾è‘—å€¼å¤§äºç­‰äºé˜ˆå€¼çš„åƒç´ ç‚¹ï¼Œæ„é€ äºŒå€¼æ©ç å›¾
    temp = torch.zeros_like(s_map)
    temp[s_map >= thresholds.view(bs, 1, 1)] = 1.0
    temp = temp.view(bs, -1)  # å±•å¹³ä¸º(batch_size, H*W)

    # è®¡ç®—True Positive (TP)ä¸False Positive (FP)ï¼š
    # æ¯å¼ å›¾åªæœ‰ä¸€ä¸ªæ­£æ ·æœ¬ï¼ŒTPä¸º1
    tp = torch.ones(bs)
    # FPä¸ºå‰©ä½™è¢«åˆ¤ä¸ºæ­£çš„ç‚¹æ•°é‡é™¤ä»¥æ€»è´Ÿæ ·æœ¬æ•°
    fp = (temp.sum(-1) - 1) / (temp.size(-1) - 1)

    # æ·»åŠ å½“å‰ç‚¹ (TP, FP) åæ ‡åˆ°AUCæ›²çº¿ç‚¹åˆ—è¡¨
    area.append(torch.stack([tp, fp.cpu()], dim=1))
    # AUCæ›²çº¿ç»ˆç‚¹ (1,1)
    area.append(torch.ones(bs, 2))
    # å°†èµ·ç‚¹ã€é˜ˆå€¼ç‚¹ã€ç»ˆç‚¹å †å æˆä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, 3, 2)
    area = torch.stack(area, dim=1)

    # åˆ©ç”¨torch.trapzè®¡ç®—AUCé¢ç§¯ï¼ˆæ¢¯å½¢ç§¯åˆ†ï¼‰
    # å¯¹TPåæ ‡(area[:,:,0])å…³äºFPåæ ‡(area[:,:,1])ç§¯åˆ†ï¼Œæ±‚å’Œè¿”å›æ€»AUC
    return torch.trapz(area[:, :, 0], area[:, :, 1]).sum()

def get_seq_score(preds, clusters, max_step, truncate_gt=False, tasks=None):
    """
    å‡è®¾ä½ æ˜¯è€å¸ˆï¼ˆæ¨¡å‹ï¼‰ï¼Œä½ è®©ä¸€ç¾¤å­¦ç”Ÿï¼ˆpredsï¼Œæ¨¡å‹é¢„æµ‹çš„æ³¨è§†è·¯å¾„ï¼‰å»çœ‹ä¸€äº›å›¾ç‰‡å¹¶è¯´å‡ºä»–ä»¬çœ‹çš„é¡ºåºï¼ˆscanpathï¼‰ã€‚
    æ¯ä¸ªå­¦ç”Ÿç»™å‡ºäº†è‡ªå·±çš„â€œæ³¨è§†é¡ºåºâ€ï¼Œä½ æ‰‹ä¸Šæœ‰æ¯å¼ å›¾çš„æ ‡å‡†ç­”æ¡ˆï¼ˆçœŸå®æ³¨è§†è·¯å¾„çš„ç±»åˆ«åºåˆ— = clustersï¼‰ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯ç»™è¿™äº›å­¦ç”Ÿæ‰“åˆ†ï¼šçœ‹ä»–ä»¬è¯´çš„æ³¨è§†è·¯å¾„å’ŒçœŸå®è·¯å¾„æœ‰å¤šåƒï¼ˆç”¨åŒ¹é…ç®—æ³• compute_SS æ¯”è¾ƒï¼‰ã€‚
    æœ€ç»ˆä½ è¦ç»Ÿè®¡å¹³å‡å¾—åˆ†ï¼Œçœ‹è¿™äº›é¢„æµ‹æ€»ä½“è¡¨ç°å¥½ä¸å¥½ã€‚
    """
    # è®¡ç®—æ¯ä¸ªé¢„æµ‹åºåˆ—ä¸çœŸå®ç±»åˆ«çš„ç›¸ä¼¼åº¦ç»“æœï¼ˆä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«scoreå’Œtaskç­‰ä¿¡æ¯ï¼‰
    results = compute_SS(preds, clusters, max_step, truncate_gt)

    if tasks is None:
        # è‹¥æœªæŒ‡å®šä»»åŠ¡åˆ—è¡¨ï¼Œç›´æ¥è¿”å›æ‰€æœ‰ç»“æœçš„scoreå¹³å‡å€¼
        return np.mean([r['score'] for r in results])
    else:
        scores = []
        # æŒ‰æ¯ä¸ªä»»åŠ¡ç­›é€‰ç»“æœï¼Œè®¡ç®—è¯¥ä»»åŠ¡å¯¹åº”çš„scoreå‡å€¼
        for task in tasks:
            scores.append(
                np.mean([r['score'] for r in results if r['task'] == task]))
        # è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºä»»åŠ¡åï¼Œå€¼ä¸ºè¯¥ä»»åŠ¡çš„å¹³å‡score
        return dict(zip(tasks, scores))


def get_semantic_seq_score(preds,
                           fixations,
                           max_step,
                           segmentation_map_dir,
                           truncate_gt=False,
                           tasks=None):
    # è°ƒç”¨ compute_SSSï¼Œä¼ å…¥é¢„æµ‹åºåˆ—ã€çœŸå®æ³¨è§†ç‚¹ã€æœ€å¤§æ­¥æ•°ã€è¯­ä¹‰åˆ†å‰²å›¾è·¯å¾„ä»¥åŠæ˜¯å¦æˆªæ–­çœŸå®åºåˆ—
    results = compute_SSS(preds, fixations, max_step, segmentation_map_dir,
                          truncate_gt)

    if tasks is None:
        # è‹¥ä¸åŒºåˆ†ä»»åŠ¡ï¼Œè¿”å›æ‰€æœ‰ç»“æœä¸­scoreçš„å¹³å‡å€¼
        return np.mean([r['score'] for r in results])
    else:
        scores = []
        # å¦‚æœæŒ‡å®šäº†ä»»åŠ¡ï¼Œåˆ™åˆ†åˆ«è®¡ç®—å„ä»»åŠ¡å¯¹åº”çš„scoreå‡å€¼
        for task in tasks:
            scores.append(
                np.mean([r['score'] for r in results if r['task'] == task]))
        # è¿”å›ä»»åŠ¡åä¸å¹³å‡scoreç»„æˆçš„å­—å…¸
        return dict(zip(tasks, scores))
